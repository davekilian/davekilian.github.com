---
layout: post
title: How Shader Transformations Work
author: Dave
draft: true
---

Shader bugs can be difficult to track down. One of the most common such bugs is
using transformations incorrectly. This bug is easy to write unless you're
careful, and it can cause all sorts of bizarre behavior:

* Lighting could look totally wrong.
* Lighting might look correct, but only from one point of view.
* It may appear as if the light source is moving with the camera.

Oftentimes beginners have a hard time figuring out which transformations to
apply to each vector. Even more advanced shader programmers get tripped up from
time to time.

This post contains both an overview of vector spaces / transformations and some
hands-on advice for writing shaders in GLSL. We'll assume you have some
experience with 3D graphics (specifically, lighting models and object
transformations). We'll also assume you have a little experience writing shader
programs.

Shall we?

## Starting with the Basics

$5 + 2 = 7$. Or does it? Things get murkier when we add units to the mix.

* If every term uses the same unit, the equation holds:
  $5 m + 2 m = 7 m$

* But if the units don't match, the equation is meaningless:
  $5 m + 2 ft = 7 (...?)$

We can, however, _transform_ each term from one unit space into another. You
can check your transformations are correct using a technique called
[dimensional analysis](http://www.chem.tamu.edu/class/fyp/mathrev/mr-da.html):

$5 m + 2 ft * \dfrac{0.3048 m}{ft} = 5.6096 m$

## Vector Spaces

Like units, vector spaces define how values may be used together. Units are for
scalar numbers; vector spaces are for vectors.

Before, we defined units relative to each other using scale factors. There were
.3048 meters in a single foot; ergo there were 0.6096 meters in two feet. We
can do the same thing (and more!) for vector spaces. A *transformation* is a
way of representing one vector space relative to another. A few common
transformations on vector spaces include:

* Scaling (multiply each component by a constant)
* Rotating (about the origin)
* Translating (add a constant to each component)
* Any combination of the above

We can represent these transformations using matrices:

1. Given a vector $v$ in vector space $A$,
2. And a matrix $M$ representing the transformation from vector space $A$ to
   vector space $B$,
3. The value of $v$ in vector space $B$ is $M \times v$, or just $Mv$.

We mentioned we could combine transformations. This works as follows:

* Say we have a vector $v$, which is defined in space $A$
* We have a matrix $M\_1$ that transforms from $A$ to space $B$
* We have a matrix $M\_2$ that transforms from $B$ to space $C$
* We would like to compute $v$ in space $C$
* To compute $v$ in $B$-space, we can multiply $v$ by $M\_1$. To compute $v$ in
  $C$-space, we can multiply the result by $M\_2$. The final expression becomes
  $M\_2(M\_1v)$, or $M\_2M\_1v$.

One handy property of matrices is associativity. This allows us to combine
multiple transformation matrices into a single transformation matrix. For
example, if we computed $M\_3 = M\_2M\_1$, then we can transform $v$ from
$A$-space to $C$-space with just one multiplication: $M\_3v$

Another handy property is invertibility. If $M\_1$ transforms from $A$-space to
$B$-space, the inverse of $M\_1$ (denoted $(M\_1)^{-1}$) transforms from
$B$-space to $A$-space.

### Why is Any of This Important?

Before, we noted that it makes no sense to directly add two numbers that have
different units. Similarly, it makes no sense to use two vectors in the same
equation (addition, subtraction, dot products, what have you) unless both
vectors are defined in the same vector space. 

## Common Vector Spaces in Graphics

In real-time rendering packages like OpenGL and DirectX, each primitive is
transformed through a series of vector spaces to end up on your screen. Here
are the more important spaces:

### Object Space

Most scenes are composed of primitives (like spheres and cylinders), models
(triangle meshes), or some combination of the two. An "object" is simply one of
these primitives or models.

Object space is a coordinate system that is convenient for defining an object
in your scene. Unlike the rest of the spaces listed below, there is more than
one object space: one for each type of object. Object spaces are usually
defined so that the object is at the origin.

### World Space

To compose objects into a scene, the objects need to be defined relative to
one another. As such, they need to each be transformed into one common vector
space. We call this space world space.

World space is typically defined so the center of the scene is at the origin.
Sometimes the units in world space correspond to physical length.

### Eye Space

(also known as camera space, view space)

The camera represents the viewer's position and orientation. Eye space is
simply the camera's object space. It is defined so that the camera is at the
origin, looking down the negative $z$ axis, with $y$ pointing up.

It turns out representing the scene in eye space makes it easier to render. As
such, we typically end up transforming everything in the scene from world space
to eye space.

### Clip Space

Clip space is defined as what the viewer sees, or more specifically, exactly
what we will be rendered to the display. 

* The origin of clip space is defined as the center of your monitor. 
* $x = -1$ corresponds to the left boundary of your monitor.
* $x = 1$ corresponds to the right boundary
* $y = -1$ corresponds to the bottom
* $y = 1$ corresponds to the top

The $z$ axis in clip space is used for depth buffering. $z=0$ correspods to the
near plane, and $z = -1$ corresponds to the far plane.

### Screen Space

Screen space is the only 2D space in this list. Screen space is defined such
that

* $(0, 0)$ corresponds to the top-left corner of your display
* $(w-1, 0)$ corresponds to the top-right
* $(0, h-1)$ corresponds to the bottom-left
* $(w-1, h-1)$ corresponds to the bottom-right

$w$ and $h$ are respecitvely the width and height of your display in 
pixels. Thus every 2D coordinate $(i, j)$ is the index of a pixel. Screen-space
coordinates are sometimes called pixel coordinates for this reason.

## Transforming Between the Spaces

Recall that transformations between vector spaces are represented by
matrices. One of the goals of rendering is to transform each triangle's
vertices from their original object-space positions to their final screen-space
pixel positions. The following transformation pipeline accomplishes this.

### Object $\rightarrow$ World

Called the *world* or *model* matrix. A composition of transformations
(typically translations, rotations and scales) that defines the location of
each object in the scene.

Since there is one object space for each object, there is also one world matrix
for each object. World matrices can be one-off computations of the output of a
specialized data structure like a 
[scene graph](http://en.wikipedia.org/wiki/Scene_graph).

### World $\rightarrow$ Eye

Called the *view* matrix. Typically a composition of translations and rotations
that move the scene so the camera (which is placed within the scene) lies where
eye space defines it to be.

That sound confusing? Here's another way to think about it. We mentioned before
that eye space is just object space for the camera (i.e. we're treating the
camera like an object). Since the camera is placed in the scene, we can 
construct a world matrix for the camera.

Such a world matrix would transform from the camera's object space (a.k.a.
eye space) to world space. With the view matrix, we wanted to do the
opposite: we wanted to transform the scene from world space to eye space.
Thus the view matrix is the inverse of the camera's world matrix.

### Eye $\rightarrow$ Clip

Called the *projection* matrix. The projection matrix is usually the
composition of a scaling operation followed by perspectivization:

* The scale operation scales down the scene so the visible range spans from -1
  to 1 on the $x$ and $y$ axes, as defined by clip space.
* The perspectivization operation "unhinges" the scene, making closer objects
  seem bigger. This simulates visual perspective.

### Clip $\rightarrow$ Screen

Usually handled by the graphics package, not exposed to the programmer. Simply
drops the $Z$ term and scales the $X$ and $Y$ terms appropriately to fill the
screen.

### Conclusion

This pipeline transforms triangle vertices from their original object-space
positions to final screen-space pixel positions. We can perform the entire
pipeline by composing the matrices from all the steps. Then we can render the
triangles as follows:

    Obtain scene global matrices (everything except the world matrix)

    For each object
        Obtain the object's world matrix
        Compose all the matrices above in the correct order

        For each triangle 
            Multiply the vertices by the final matrix
            Figure out which screen pixels the vertices enclose (rasterization)
            Figure out what colors to assign those pixels (shading)

In essence, this how GPUs render scenes.

## Transformation Matrices in OpenGL

The rest of this post will focus on using the OpenGL compatibility profile (with 
`glPushMatrix()` and friends) to write shaders.

### OpenGL's Matrix Model

OpenGL lets you manipulate two matrices: the modelview matrix and the
projection matrix. 

* The modelview matrix is the amalgamation of each object's world matrix with 
  the camera's view matrix. In other words, modelview = view $\times$ model.
  Hence the name. As such, the modelview transforms object space to eye space.

* The projection matrix defines the transformation from eye space to clip
  space.

To transform from object space to clip space, you can multiply a vertex by
$projection \times modelview = projection \times view \times model$. 
This is sometimes called the ModelViewProjection matrix.

### Matrix Stack

OpenGL provides a matrix 'stack' to hold intermediate transformations. It's not
actually a stack _per se_, but it does provide FIFO ordering. The last 
transformation you specify on the OpenGL stack is the first that gets applied 
to the vertex.

There are two matrix stacks: one for the projection matrix and one for the
modelview. The projection stack is typically boring, so we'll focus on the 
modelview stack.

Why a stack? Think about the order vertices are multiplied by matrices:

1. The original vertex is multiplied by the world matrix, moving it to world space
2. The new vertex is multiplied by the view matrix, moving it to eye space

Note that the view matrix changes once (it's constant throughout the scene),
but the world matrix changes often (changes on a per-object basis). This is
what makes the stack useful: we can push the view matrix once, then push/pop
the world matrix once per object. Our transformation algorithm becomes:

    Switch to the projection stack
    Clear the stack
    Push the projection matrix

    Switch to the modelview stack
    Clear the stack
    Push the view matrix

    For each object
        Push the object's model matrix
        Render the object
        Pop the object's model matrix

One significant pitfall of the matrix stack is ALL operations must be specified
in FIFO order. So if you want to specify an object's model matrix as

1. Rotate 90 degrees
2. Translate 3 units up

you would need to reverse the `gl` calls:

    glTranslate(...);
    glRotate(...);

### Vertex and Fragment Shaders

Shaders are programs you can run on the GPU to perform certain steps of the
[rendering pipeline](http://www.opengl.org/wiki/Rendering_Pipeline_Overview).

The vertex shader is executed on each input vertex. It is given a vertex in
object space and is responsible for transforming the vertex to clip space. 

The graphics hardware moves the vertex from clip space to screen space, and
then rasterizes to identify which pixels are inside the screen-space triangle.

The fragment shader is then executed on each rasterized pixel, to determine
what color it should have.

This is a simplified view of the rendering pipeline, but it will suffice for
our discussion.

## Lighting and Transformations

Determing light colors involves several vector operations. For example, the
diffuse intensity of a pixel depends on the dot product between the normal
vector $N$ and the vector to the light, $L$. 

As per our discussion earlier in this article, $N$ and $L$ would have to be in
the same vector space for their dot product to have any meaning. One natural
question, then, is which space should we transform them to?

Typically world space and eye space are good candidates, as they are
scene-global and are reasonably easy to visualize. Unfortunately, transforming
to either is made difficult by the way OpenGL combines the model and view
matrices into one modelview matrix.

### The OpenGL Way

Consider Phong's lighting model for a single point light:

$color = ambient + diffuse * (N \cdot L) + specular * (E \cdot R)^{specular\\\_power}$

We have four vector to compute:

* $N$, the normal vector
* $L$, the vector from the vertex to the light source
* $E$, the vector from the vertex to the eye point
* $R$, which is $L$ reflected about $N$

We'll represent all the vectors in eye space, as OpenGL makes this the easiest
space to transform to.

* To compute $N$, we'll multiply the object-space normal by the modelview
  matrix. In GLSL, this is easy:

        vec3 N = gl_NormalMatrix * gl_Normal;

* To compute $L$, we need to know the vertex position and the light position,
  both in eye space. The problem then comes down to vector subtraction:

        vec4 vertex = gl_ModelViewMatrix * gl_Vertex;
        vec4 light = gl_LightSource[0].position;
        vec3 L = normalize(light - vertex).xyz;

* To compute $E$:

        vec3 eyepos = vec3(0.0); // by definition of eye space
        vec3 E = eyepos - vertex; 
        
        // or just
        vec3 E = -vertex.xyz;

* To compute $R$:

        vec3 R = reflect(L, N);

An astute reader might wonder how `gl_LightSource[0]` gets set in your C
application. When you call `glLightfv(GL_POSITION, ...)`, you specify the
light's position in world space; yet the light pops out in the shader in eye
space. How can this be?

When you call `glLightfv(GL_POSITION, ...)` OpenGL actually multiplies the
position you specified by the current modelview matrix. The assumption is
that, when you specify your lights, the modelview matrix == the view matrix.
Thus multiplying moves the light from world space to eye space.

That's why it's very important your draw loop takes the following form:

    Switch to the modelview stack
    Clear the stack
    Push the view matrix

    Set up your lights (again, even if they're not moving!)

    For each object
        Push the model matrix
        Render the object
        Pop the model matrix

If you swap the order of these operations, you'll likely get incorrect
transformations and a nice headache.

## Wrap-Up

Much of the OpenGL API was deprecated in OpenGL 3.0 and 'removed' in OpenGL
3.1. This functionality, which we used above extensively, is still available on
desktop computers via what OpenGL calls the 'compatability profile.'

If you opt for the core profile instead (or if you're using OpenGL ES or WebGL,
neither of which supports a compatibility profile), you have much more freedom. 
You can specify your model, view and projection matrices to your shaders as
separate uniforms. This freedom comes at the cost of boilerplate: you have to 
do much more work to render just a single triangle. 

Eventually, no matter what platform you're writing shaders on, being able to
think about your vectors in terms of spaces and transformations will come in
handy. This article should hopefully have been enough to get you started!

