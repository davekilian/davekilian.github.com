---
layout: post
title: How Shader Transformations Work
author: Dave
draft: true
---

Debugging shaders is hard. Without printlines or a proper debugger, shader
developers often resort to using the fragment shader's output to see what's
going on. This way of debugging is slow and unproductive. For most people, it's
hard to make sense of per-pixel color output.

It would be nice to not have to debug your shaders in the first place, if it
can be helped ;). This post focuses on the most common shader programming
mistake: messing up your transformations.

## Symptoms

The kind of behavior you can get from mishandling transformations is bizarre
and varied:

* Lighting could be totally wrong or missing.
* Lighting might look correct, but only from one point of view.
* Lighting may look correct, but the lights might seem to follow the camera.

To understand why this kind of thing happens, let's take a look at something a
bit easier to grasp than shaders.

## 'Transforming' Physical Units

$5 + 2 = 7$. 
That's clearly correct, right? Who would dispute that?

Me.

* The equation works if every term uses the same unit:
  $5 m + 2 m = 7 m$
* But if the units don't match, the equation is meaningless:
  $5 m + 2 ft = 7 (...?)$

In short, the math only makes sense if every term uses the same unit.
In high school physics you probably learned what to do if your terms' units
mismatch: convert (or _transform_) some of the terms to a different unit of
measurement. Then the math works out fine.

$5 m + 2 ft * \dfrac{0.3048 m}{ft} = 5 m + 0.6096 m = 5.6096 m$

## Vector Spaces are the Same

You might say a vector space is to a vector what a unit is to a number. If you
have an equation made of vector addition, subtraction, dot products, or
what-have-you, the math only works out if the vectors all occupy the same
vector space. 

Again like units, you can use a _transformation_ to convert some vectors
to a different vector space. Once everything is in the same vector space, the
math works the way it's supposed to and your shaders are bug-free. Hooray!

Of course, this is easier said than done. Once we understand how
transformations tie into the graphics pipeline, however, we can simplify the
process by setting up conventions and best-practices.

## Vector Space Transformations

You've probably seen these before. They allow you to scale, rotate and
translate the objects in your scene.

Mathematically, each transformation is represented by a matrix. Say you have a
vector in the vector space $A$ ("$A$-space" for short), and you'd like to
transform it to $B$-space. If you already have the transformation matrix 
$M_{AB}$, you can simply multiply it with the $A$-space vector. The
result is the $B$-space vector. So, $v_B = M_{AB} \times v_A$.

Often $M_{AB}$ is simple. For example, if $B$-space is the same as $A$-space,
except that everything in $B$-space is twice as large as in $A$-space, then
$M_{AB}$ is just a scale matrix.
You can build complex $M_{AB}$'s by daisy-chaining the simpler ones.
They are combined via matrix multiplication:

* If $M\_{AB}$ transforms from $A$-space to $B$-space
* And $M\_{BC}$ transforms from $B$-space to $C$-space
* Then $M\_{AC} = M_{BC} \times M_{AB}$ transforms from $A$-space to $C$-space.

Intuitively, you might say $M_{AC}$ transforms from $A$-space to $B$-space to
$C$-space. 

## Transformations in Graphics

TODO move the discussion about the pipeline here. Talk about why precomputing
everything down to a single matrix is a big win from a Big-O standpoint.

:qa
## Common Vector Spaces in Graphics

In real-time rendering packages like OpenGL and DirectX, each primitive is
transformed through a series of vector spaces to end up on your screen. Here
are the more important spaces:

### Object Space

Most scenes are composed of primitives (like spheres and cylinders), models
(triangle meshes), or some combination of the two. An "object" is simply one of
these primitives or models.

Object space is a coordinate system that is convenient for defining an object
in your scene. Unlike the rest of the spaces listed below, there is more than
one object space: one for each type of object. Object spaces are usually
defined so that the object is at the origin.

### World Space

To compose objects into a scene, the objects need to be defined relative to
one another. As such, they need to each be transformed into one common vector
space. We call this space world space.

World space is typically defined so the center of the scene is at the origin.
Sometimes the units in world space correspond to physical length.

### Eye Space

(also known as camera space, view space)

The camera represents the viewer's position and orientation. Eye space is
simply the camera's object space. It is defined so that the camera is at the
origin, looking down the negative $z$ axis, with $y$ pointing up.

It turns out representing the scene in eye space makes it easier to render. As
such, we typically end up transforming everything in the scene from world space
to eye space.

### Clip Space

Clip space is defined as what the viewer sees, or more specifically, exactly
what we will be rendered to the display. 

* The origin of clip space is defined as the center of your monitor. 
* $x = -1$ corresponds to the left boundary of your monitor.
* $x = 1$ corresponds to the right boundary
* $y = -1$ corresponds to the bottom
* $y = 1$ corresponds to the top

The $z$ axis in clip space is used for depth buffering. $z=0$ correspods to the
near plane, and $z = -1$ corresponds to the far plane.

### Screen Space

Screen space is the only 2D space in this list. Screen space is defined such
that

* $(0, 0)$ corresponds to the top-left corner of your display
* $(w-1, 0)$ corresponds to the top-right
* $(0, h-1)$ corresponds to the bottom-left
* $(w-1, h-1)$ corresponds to the bottom-right

$w$ and $h$ are respecitvely the width and height of your display in 
pixels. Thus every 2D coordinate $(i, j)$ is the index of a pixel. Screen-space
coordinates are sometimes called pixel coordinates for this reason.

## Transforming Between the Spaces

Recall that transformations between vector spaces are represented by
matrices. One of the goals of rendering is to transform each triangle's
vertices from their original object-space positions to their final screen-space
pixel positions. The following transformation pipeline accomplishes this.

### Object $\rightarrow$ World

Called the *world* or *model* matrix. A composition of transformations
(typically translations, rotations and scales) that defines the location of
each object in the scene.

Since there is one object space for each object, there is also one world matrix
for each object. World matrices can be one-off computations of the output of a
specialized data structure like a 
[scene graph](http://en.wikipedia.org/wiki/Scene_graph).

### World $\rightarrow$ Eye

Called the *view* matrix. Typically a composition of translations and rotations
that move the scene so the camera (which is placed within the scene) lies where
eye space defines it to be.

That sound confusing? Here's another way to think about it. We mentioned before
that eye space is just object space for the camera (i.e. we're treating the
camera like an object). Since the camera is placed in the scene, we can 
construct a world matrix for the camera.

Such a world matrix would transform from the camera's object space (a.k.a.
eye space) to world space. With the view matrix, we wanted to do the
opposite: we wanted to transform the scene from world space to eye space.
Thus the view matrix is the inverse of the camera's world matrix.

### Eye $\rightarrow$ Clip

Called the *projection* matrix. The projection matrix is usually the
composition of a scaling operation followed by perspectivization:

* The scale operation scales down the scene so the visible range spans from -1
  to 1 on the $x$ and $y$ axes, as defined by clip space.
* The perspectivization operation "unhinges" the scene, making closer objects
  seem bigger. This simulates visual perspective.

### Clip $\rightarrow$ Screen

Usually handled by the graphics package, not exposed to the programmer. Simply
drops the $Z$ term and scales the $X$ and $Y$ terms appropriately to fill the
screen.

### Conclusion

This pipeline transforms triangle vertices from their original object-space
positions to final screen-space pixel positions. We can perform the entire
pipeline by composing the matrices from all the steps. Then we can render the
triangles as follows:

    Obtain scene global matrices (everything except the world matrix)

    For each object
        Obtain the object's world matrix
        Compose all the matrices above in the correct order

        For each triangle 
            Multiply the vertices by the final matrix
            Figure out which screen pixels the vertices enclose (rasterization)
            Figure out what colors to assign those pixels (shading)

In essence, this how GPUs render scenes.

## Transformation Matrices in OpenGL

The rest of this post will focus on using the OpenGL compatibility profile (with 
`glPushMatrix()` and friends) to write shaders.

### OpenGL's Matrix Model

OpenGL lets you manipulate two matrices: the modelview matrix and the
projection matrix. 

* The modelview matrix is the amalgamation of each object's world matrix with 
  the camera's view matrix. In other words, modelview = view $\times$ model.
  Hence the name. As such, the modelview transforms object space to eye space.

* The projection matrix defines the transformation from eye space to clip
  space.

To transform from object space to clip space, you can multiply a vertex by
$projection \times modelview = projection \times view \times model$. 
This is sometimes called the ModelViewProjection matrix.

### Matrix Stack

OpenGL provides a matrix 'stack' to hold intermediate transformations. It's not
actually a stack _per se_, but it does provide FIFO ordering. The last 
transformation you specify on the OpenGL stack is the first that gets applied 
to the vertex.

There are two matrix stacks: one for the projection matrix and one for the
modelview. The projection stack is typically boring, so we'll focus on the 
modelview stack.

Why a stack? Think about the order vertices are multiplied by matrices:

1. The original vertex is multiplied by the world matrix, moving it to world space
2. The new vertex is multiplied by the view matrix, moving it to eye space

Note that the view matrix changes once (it's constant throughout the scene),
but the world matrix changes often (changes on a per-object basis). This is
what makes the stack useful: we can push the view matrix once, then push/pop
the world matrix once per object. Our transformation algorithm becomes:

    Switch to the projection stack
    Clear the stack
    Push the projection matrix

    Switch to the modelview stack
    Clear the stack
    Push the view matrix

    For each object
        Push the object's model matrix
        Render the object
        Pop the object's model matrix

One significant pitfall of the matrix stack is ALL operations must be specified
in FIFO order. So if you want to specify an object's model matrix as

1. Rotate 90 degrees
2. Translate 3 units up

you would need to reverse the `gl` calls:

    glTranslate(...);
    glRotate(...);

### Vertex and Fragment Shaders

Shaders are programs you can run on the GPU to perform certain steps of the
[rendering pipeline](http://www.opengl.org/wiki/Rendering_Pipeline_Overview).

The vertex shader is executed on each input vertex. It is given a vertex in
object space and is responsible for transforming the vertex to clip space. 

The graphics hardware moves the vertex from clip space to screen space, and
then rasterizes to identify which pixels are inside the screen-space triangle.

The fragment shader is then executed on each rasterized pixel, to determine
what color it should have.

This is a simplified view of the rendering pipeline, but it will suffice for
our discussion.

## Lighting and Transformations

Determing light colors involves several vector operations. For example, the
diffuse intensity of a pixel depends on the dot product between the normal
vector $N$ and the vector to the light, $L$. 

As per our discussion earlier in this article, $N$ and $L$ would have to be in
the same vector space for their dot product to have any meaning. One natural
question, then, is which space should we transform them to?

Typically world space and eye space are good candidates, as they are
scene-global and are reasonably easy to visualize. Unfortunately, transforming
to either is made difficult by the way OpenGL combines the model and view
matrices into one modelview matrix.

### The OpenGL Way

Consider Phong's lighting model for a single point light:

$color = ambient + diffuse * (N \cdot L) + specular * (E \cdot R)^{specular\\\_power}$

We have four vector to compute:

* $N$, the normal vector
* $L$, the vector from the vertex to the light source
* $E$, the vector from the vertex to the eye point
* $R$, which is $L$ reflected about $N$

We'll represent all the vectors in eye space, as OpenGL makes this the easiest
space to transform to.

* To compute $N$, we'll multiply the object-space normal by the modelview
  matrix. In GLSL, this is easy:

        vec3 N = gl_NormalMatrix * gl_Normal;

* To compute $L$, we need to know the vertex position and the light position,
  both in eye space. The problem then comes down to vector subtraction:

        vec4 vertex = gl_ModelViewMatrix * gl_Vertex;
        vec4 light = gl_LightSource[0].position;
        vec3 L = normalize(light - vertex).xyz;

* To compute $E$:

        vec3 eyepos = vec3(0.0); // by definition of eye space
        vec3 E = eyepos - vertex; 
        
        // or just
        vec3 E = -vertex.xyz;

* To compute $R$:

        vec3 R = reflect(L, N);

An astute reader might wonder how `gl_LightSource[0]` gets set in your C
application. When you call `glLightfv(GL_POSITION, ...)`, you specify the
light's position in world space; yet the light pops out in the shader in eye
space. How can this be?

When you call `glLightfv(GL_POSITION, ...)` OpenGL actually multiplies the
position you specified by the current modelview matrix. The assumption is
that, when you specify your lights, the modelview matrix == the view matrix.
Thus multiplying moves the light from world space to eye space.

That's why it's very important your draw loop takes the following form:

    Switch to the modelview stack
    Clear the stack
    Push the view matrix

    Set up your lights (again, even if they're not moving!)

    For each object
        Push the model matrix
        Render the object
        Pop the model matrix

If you swap the order of these operations, you'll likely get incorrect
transformations and a nice headache.

## Wrap-Up

Much of the OpenGL API was deprecated in OpenGL 3.0 and 'removed' in OpenGL
3.1. This functionality, which we used above extensively, is still available on
desktop computers via what OpenGL calls the 'compatability profile.'

If you opt for the core profile instead (or if you're using OpenGL ES or WebGL,
neither of which supports a compatibility profile), you have much more freedom. 
You can specify your model, view and projection matrices to your shaders as
separate uniforms. This freedom comes at the cost of boilerplate: you have to 
do much more work to render just a single triangle. 

Eventually, no matter what platform you're writing shaders on, being able to
think about your vectors in terms of spaces and transformations will come in
handy. This article should hopefully have been enough to get you started!

