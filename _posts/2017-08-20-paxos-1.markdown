---
layout: post
title: Learning Distributed System Design with Paxos
author: Dave
draft: true
---

The modern distributed systems programmer has a glut of tools to choose from, thanks to decades of research and development that have been fueled by seemingly never-ending growth in the Internet and the cloud.
Many distributed systems engineers do great work just by learning these tools and combining them into fully-functioning solutions.
The very best engineers prefer to know the inner workings of their tools, in order to best play to each tool's strengths and avoid its weaknesses.

If you're interested in learning how many distributed programming tools work, or how to design your own, there's no better place to start than learning the inner workings of Paxos.
Designing and implementing Paxos exemplifies many of the problems distributed systems designers face day-to-day, along with many techniques commonly used for solving them.
Paxos is also extremely popular across the industry, especially in distributed storage and databases, so if you're interested in the subject you'll likely need to learn it sooner or later anyway.

In this blog series, we'll derive Paxos from first principles.
Unlike many existing derivations, this one will be engineering-focused: instead of verbose specifications and exacting proofs of correctness, we'll concentrate on motivation, understanding, and real-world practices and techniques.
Our goal will be less to provide a ready-made implementation of Paxos, and more to give you a taste of distributed programming problems and solutions.

If you've heard of Paxos before, you probably know of its reputation for being complex, hard to understand, and tricky to implement, which would make it an odd choice to use for use in an introductory-level text.
Rather than subjecting the reading to a trial by fire, however, we aim to demonstrate that Paxos is really not so complicated a solution to arrive at, at least once you have the right mindset and toolbox.
After all, Lamport himself claims that

> _[...Paxos] is among the simplest and most obvious of distributed algorithms. \[...\] This consensus algorithm follows almost unavoidably from the properties we want it to satisfy._
> 
> &mdash; [Paxos Made Simple](https://www.microsoft.com/en-us/research/publication/paxos-made-simple/), Lamport '01

So without further ado, let's begin!

## A Brief History

Despite the number of tools and techniques that have evolved over the years, distributed system design still largely rests on many of the same theoretical foundations that were laid decades ago.
One of the chief among those is the distributed consensus algorithm Paxos.

Paxos was originally developed by the distributed systems researcher Leslie Lamport in the late 1980s/early 1990s.
Lamport tried to publish it in 1990 in his paper [The Part-Time Parliament](TODO), but was unsuccessful in doing so.
Few researchers at the time understood the paper and its relevance, in no small part because of the format Lamport used.
By describing his algorithm as if it were invented in ancient times by a fictional government on the Greek island of Paxos, Lamport made the algorithm more approachable, but its relevance to the field more obscure.
It took nearly a decade and the implementation of several Paxos-based systems for the paper to finally be reviewed, accepted, and published in 1998.
It has since become one of the field's most famous results.

A few years later, Lamport published [Paxos Made Simple](TODO), which describes the same algorithm without the Greek allegory, instead opting for a simpler, more direct approach.

In the intervening years, Paxos has become ubiquitous across the industry.
Most distributed storage / database engines incorporate in their implementation Paxos or a Paxos-based algorithm.

More recently, in 2013, Ongaro and Ousterhout published [Raft](https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf), a reimagining of Paxos aimed at addressing some of the algorithm's complexities and making it easier to understand and reason about.
Raft has recently been picking up steam as a possible successor to Paxos, replacing it in several distributed system courses at well-known universities and in a few production systems.

## The Consensus Problem

Paxos implements a solution to *distributed consensus*, one of the foundational problems of implementing a distributed system.
Distributed consensus can be summarized as thus: given a network of nodes running instances of a program, how can all the instances of the program agree on the value of a variable?
A consensus algorithm is a useful primitive in the distributed systems designer's toolbox: if you can solve consensus well, you can use that to implement more useful higher-level tools.

---

## TODO

Take a look at the older content below and refine our plan for deriving the algorithm.
Along the way, look out for techniques that need to be discussed separately.
Then come up with a final order of topics.

The old stuff:

Build as follows:

1. Most trivial case: one server does everything
2. For failures, etc, would be nice to have *replication*
3. Trivial replication: one master, many slaves
4. Network unreliable, so master has to retry forever
5. We want to promote a slave to a master if the master fails
6. Assuming we can do that, network unreliable, leads to split brain
7. Somehow have to handle multiple nodes that think they're 'masters'
8. Master nodes ask a majority of slaves for current value, propagate latest
9. Result: once the majority of slaves agree, masters keep propagating
10. Result: a master which cannot reach a majority of slaves cannot progress
11. But alas, this only works if you have a global synchronous clock for rounds
12. Instead, use 'promises' to prevent the system from backtracking
13. 'Warring proposers' problem, need to back off somehow

I think we now have Synod?
Reread the paper to be sure.
I feel like I at least forgot to include something about learners.

Post II will be the full Parliament algorithm.
Make an endless list of decisions, keeping leadership stable via leases.

Then, as a third post, using this as a general distributed state mechanism.
Chubby lock service, stuff like that.
Also shortcomings, like how to add or remove nodes.

In the fourth post, a working demo.

